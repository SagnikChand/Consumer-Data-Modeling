{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "In this project, we delve into a comprehensive data analysis and modeling task centered around customer data from the Adventure Works Cycles company. The primary objective is to leverage the demographic and purchase data collected to build predictive models that can provide actionable insights for the company. The challenge is structured to put into practice key principles and techniques learned throughout the course, requiring interpretation of requirements, performance of necessary tasks, and optimization of models to achieve the best results.\n",
        "\n",
        "### Dataset Overview\n",
        "\n",
        "The data used in this project is composed of three distinct files, each containing crucial information about Adventure Works Cycles' customers:\n",
        "\n",
        "1. **AdvWorksCusts.csv**: This file contains detailed demographic data for each customer, including:\n",
        "   - **CustomerID**: Unique identifier for each customer.\n",
        "   - **Title, FirstName, MiddleName, LastName, Suffix**: Personal details of the customer.\n",
        "   - **AddressLine1, AddressLine2, City, StateProvince, CountryRegion, PostalCode**: Address details of the customer.\n",
        "   - **PhoneNumber**: Contact number of the customer.\n",
        "   - **BirthDate**: Date of birth of the customer.\n",
        "   - **Education**: Education level of the customer.\n",
        "   - **Occupation**: Job type of the customer.\n",
        "   - **Gender**: Gender of the customer.\n",
        "   - **MaritalStatus**: Marital status of the customer.\n",
        "   - **HomeOwnerFlag**: Indicates whether the customer owns a home.\n",
        "   - **NumberCarsOwned, NumberChildrenAtHome, TotalChildren**: Details about the customer's family.\n",
        "   - **YearlyIncome**: Annual income of the customer.\n",
        "\n",
        "2. **AW_AveMonthSpend.csv**: This file contains sales data for existing customers, specifically:\n",
        "   - **CustomerID**: Unique identifier for each customer.\n",
        "   - **AveMonthSpend**: Average monthly spend of the customer with Adventure Works Cycles.\n",
        "\n",
        "3. **AW_BikeBuyer.csv**: This file also contains sales data, indicating:\n",
        "   - **CustomerID**: Unique identifier for each customer.\n",
        "   - **BikeBuyer**: A flag indicating whether the customer has previously purchased a bike.\n",
        "\n",
        "### Project Purpose\n",
        "\n",
        "The primary goals of this project are as follows:\n",
        "\n",
        "1. **Explore Customer Data**: Gain insights into customer characteristics and purchasing behavior.\n",
        "2. **Build Regression Model**: Predict the average monthly spend of a customer based on demographic and purchase data.\n",
        "\n",
        "### Workflow Process\n",
        "\n",
        "The project is divided into several key phases, each focusing on different aspects of the data analysis and modeling process:\n",
        "\n",
        "1. **Data Exploration**:\n",
        "   - Load and inspect the datasets to understand the structure and content.\n",
        "   - Perform exploratory data analysis (EDA) to identify patterns, correlations, and potential anomalies.\n",
        "   - Visualize the data to gain deeper insights into customer demographics and purchasing behavior.\n",
        "\n",
        "2. **Data Preprocessing**:\n",
        "   - Clean the datasets by handling missing values, removing duplicates, and correcting data types.\n",
        "   - Merge the datasets based on the common `CustomerID` field to create a comprehensive dataset.\n",
        "   - Feature engineering to create new variables that may enhance the predictive power of the models.\n",
        "\n",
        "3. **Model Building**:\n",
        "   - **Regression Model**: Predict the average monthly spend of customers.\n",
        "     - Split the data into training and testing sets.\n",
        "     - Train various regression models (e.g., linear regression, decision trees, random forests).\n",
        "     - Evaluate model performance using metrics such as mean absolute error (MAE) and root mean squared error (RMSE).\n",
        "     - Optimize the best-performing model using hyperparameter tuning.\n",
        "\n",
        "4. **Model Testing and Prediction**:\n",
        "   - Apply the trained models to the test datasets to predict the outcomes for new customers.\n",
        "   - Use the regression model to predict average monthly spend for new customers, evaluating the model's predictive accuracy.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This project demonstrates the application of key data analysis and machine learning techniques to real-world customer data. By exploring the data, building predictive models, and evaluating their performance, we aim to provide valuable insights to Adventure Works Cycles, aiding in strategic decision-making and enhancing customer relationships. The challenge underscores the importance of a systematic approach to data preprocessing, model building, and evaluation, ensuring robust and reliable predictive outcomes."
      ],
      "metadata": {
        "id": "l-Vu7vdrLEvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration Phase\n",
        "\n",
        "The data exploration phase involves understanding the structure and characteristics of the datasets. We'll load the datasets, inspect their contents, check for missing values, and perform basic statistical analysis to gain initial insights."
      ],
      "metadata": {
        "id": "jSMUoIzwPdgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Import Necessary Libraries"
      ],
      "metadata": {
        "id": "Jhy-H9hiPYF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "nWW9DO0rPWTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Load the Datasets"
      ],
      "metadata": {
        "id": "XgbY1SZuPRqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "customers_df = pd.read_csv('AdvWorksCusts.csv')\n",
        "monthly_spend_df = pd.read_csv('AW_AveMonthSpend.csv')\n",
        "bike_buyer_df = pd.read_csv('AW_BikeBuyer.csv')"
      ],
      "metadata": {
        "id": "OVbhXH_BPQe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Inspect the Data"
      ],
      "metadata": {
        "id": "1oCrBP1mPJXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. View the First Few Rows: Check the first few rows of each dataset to get an initial understanding of the data.\n",
        "\n",
        "print(customers_df.head())\n",
        "print(monthly_spend_df.head())\n",
        "print(bike_buyer_df.head())\n",
        "\n",
        "#2. Summary Statistics: Get a summary of each dataset, including basic statistics for numeric columns.\n",
        "\n",
        "print(customers_df.describe())\n",
        "print(monthly_spend_df.describe())\n",
        "print(bike_buyer_df.describe())\n",
        "\n",
        "#3. Information and Data Types: Check the data types of each column and the presence of null values.\n",
        "\n",
        "print(customers_df.info())\n",
        "print(monthly_spend_df.info())\n",
        "print(bike_buyer_df.info())\n",
        "\n",
        "#4. Missing Values: Identify columns with missing values.\n",
        "\n",
        "print(customers_df.isnull().sum())\n",
        "print(monthly_spend_df.isnull().sum())\n",
        "print(bike_buyer_df.isnull().sum())"
      ],
      "metadata": {
        "id": "Bw5YhfebO5YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "o5GF-0C2Oi6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Distribution of Demographic Features: Visualize the distribution of key demographic features using histograms and bar plots.\n",
        "\n",
        "# Histogram of Age\n",
        "customers_df['BirthDate'] = pd.to_datetime(customers_df['BirthDate'])\n",
        "customers_df['Age'] = (pd.to_datetime('1998-01-01') - customers_df['BirthDate']).dt.days // 365\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(customers_df['Age'], bins=30, kde=True)\n",
        "plt.title('Distribution of Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot of Education levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=customers_df, x='Education')\n",
        "plt.title('Distribution of Education Levels')\n",
        "plt.xlabel('Education Level')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Bar plot of Occupation types\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=customers_df, x='Occupation')\n",
        "plt.title('Distribution of Occupation Types')\n",
        "plt.xlabel('Occupation')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "#2. Correlation Analysis: Analyze the correlation between numeric features to identify potential relationships.\n",
        "\n",
        "# Correlation matrix\n",
        "corr_matrix = customers_df.corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "#3. Distribution of Target Variables: Visualize the distribution of the target variables (BikeBuyer and AveMonthSpend).\n",
        "\n",
        "# Histogram of Average Monthly Spend\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(monthly_spend_df['AveMonthSpend'], bins=30, kde=True)\n",
        "plt.title('Distribution of Average Monthly Spend')\n",
        "plt.xlabel('Average Monthly Spend')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot of Bike Buyer\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=bike_buyer_df, x='BikeBuyer')\n",
        "plt.title('Distribution of Bike Buyer')\n",
        "plt.xlabel('Bike Buyer')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3FZjCPqIOag_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description\n",
        "\n",
        "1. **Loading Data**: We start by loading the datasets using `pandas`. The `pd.read_csv()` function is used to read CSV files into DataFrames.\n",
        "\n",
        "2. **Inspecting Data**: We use `head()`, `describe()`, and `info()` functions to get an initial overview of the data, including the first few rows, summary statistics, and data types.\n",
        "\n",
        "3. **Checking Missing Values**: The `isnull().sum()` function helps us identify columns with missing values, which is crucial for data cleaning.\n",
        "\n",
        "4. **Exploratory Data Analysis (EDA)**:\n",
        "   - We visualize the distribution of key demographic features (Age, Education, and Occupation) using histograms and bar plots to understand the data better.\n",
        "   - The correlation matrix is used to identify relationships between numeric features.\n",
        "   - We examine the distribution of target variables (Average Monthly Spend and Bike Buyer) to understand their characteristics.\n",
        "\n",
        "With these initial insights, we can proceed to the data preprocessing phase, where we will clean the data, handle missing values, and prepare it for model building."
      ],
      "metadata": {
        "id": "1W58QqBjOL9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing Phase\n",
        "\n",
        "The preprocessing phase involves preparing the data for analysis and modeling. This includes cleaning the data, handling missing values, merging datasets, and creating new features that may enhance the predictive power of our models. Below is the code and its detailed description for the preprocessing steps performed:"
      ],
      "metadata": {
        "id": "7av_LqLqNjLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Step 1: Import Necessary Libraries\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, date\n",
        "\n",
        "#### Step 2: Load the Datasets\n",
        "\n",
        "#We start by loading the three datasets: customer demographics, average monthly spend, and bike buyer data.\n",
        "\n",
        "AdvWorksCusts = pd.read_csv('AdvWorksCusts.csv')\n",
        "AW_BikeBuyer = pd.read_csv('AW_BikeBuyer.csv')\n",
        "AW_AveMonthSpend = pd.read_csv('AW_AveMonthSpend.csv')\n",
        "\n",
        "#### Step 3: Remove Duplicates\n",
        "\n",
        "#To ensure data integrity, we remove any duplicate entries based on the `CustomerID` column.\n",
        "\n",
        "AdvWorksCusts.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)\n",
        "AW_BikeBuyer.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)\n",
        "AW_AveMonthSpend.drop_duplicates(subset=['CustomerID'], keep='first', inplace=True)\n",
        "\n",
        "#### Step 4: Merge Datasets\n",
        "\n",
        "#We merge the three datasets into a single DataFrame using the `CustomerID` column as the key.\n",
        "\n",
        "df = pd.merge(AdvWorksCusts, AW_AveMonthSpend, on='CustomerID')\n",
        "df = pd.merge(df, AW_BikeBuyer, on='CustomerID')\n",
        "\n",
        "#### Step 5: Drop Irrelevant Columns\n",
        "\n",
        "#Certain columns are not needed for our analysis and modeling, so we drop them to simplify the dataset.\n",
        "\n",
        "df.drop(['Title', 'FirstName', 'LastName', 'MiddleName', 'Suffix', 'AddressLine1',\n",
        "         'AddressLine2', 'StateProvinceName', 'PhoneNumber', 'City', 'PostalCode'], axis=1, inplace=True)\n",
        "\n",
        "#### Step 6: Calculate Age\n",
        "\n",
        "#We calculate the age of each customer based on their birthdate and the data collection date (January 1, 1998). We then convert the age from days to years and drop the intermediate columns used in this calculation.\n",
        "\n",
        "df['BirthDate'] = pd.to_datetime(df['BirthDate'])\n",
        "df['Coll_date'] = date(year=1998, month=1, day=1)\n",
        "df['Coll_date'] = pd.to_datetime(df['Coll_date'])\n",
        "df['Age_days'] = df['Coll_date'] - df['BirthDate']\n",
        "df['Age'] = df['Age_days'].astype('timedelta64[Y]')\n",
        "df.drop(['Coll_date', 'Age_days', 'BirthDate'], axis=1, inplace=True)\n",
        "df['Age'] = df['Age'].astype(int)\n",
        "\n",
        "#### Step 7: Drop Unnecessary Columns\n",
        "\n",
        "#We drop additional columns that are not needed for our regression model, such as `HomeOwnerFlag` and `BikeBuyer`.\n",
        "\n",
        "df.drop(['HomeOwnerFlag', 'BikeBuyer'], axis=1, inplace=True)\n",
        "\n",
        "#### Step 8: Create New Features\n",
        "\n",
        "#We create a new feature `ChildrenOut`, which represents the number of children not living at home by subtracting `NumberChildrenAtHome` from `TotalChildren`.\n",
        "\n",
        "df['ChildrenOut'] = df['TotalChildren'] - df['NumberChildrenAtHome']\n",
        "\n",
        "#### Step 9: Inspect the Processed Data\n",
        "\n",
        "#We use the `info()` method to check the structure and data types of the processed DataFrame.\n",
        "\n",
        "df.info()\n",
        "\n",
        "#### Step 10: Save the Processed Data\n",
        "\n",
        "#Finally, we save the cleaned and processed DataFrame to a new CSV file for use in the modeling phase.\n",
        "\n",
        "df.to_csv('Reg_AveMonthSpend.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "SN8SQ5cOQMEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description\n",
        "\n",
        "1. **Loading Data**: We load the customer demographic, average monthly spend, and bike buyer data into three separate DataFrames using `pd.read_csv()`.\n",
        "\n",
        "2. **Removing Duplicates**: We remove duplicate rows based on the `CustomerID` column to ensure each customer is represented only once in each dataset.\n",
        "\n",
        "3. **Merging Datasets**: The datasets are merged into a single DataFrame using `CustomerID` as the key. This creates a comprehensive dataset that includes demographic data, average monthly spend, and bike buying behavior for each customer.\n",
        "\n",
        "4. **Dropping Irrelevant Columns**: We drop columns that are not needed for our analysis and modeling, such as personal details and address information.\n",
        "\n",
        "5. **Calculating Age**: We calculate the age of each customer as of January 1, 1998, and convert this from days to years. Intermediate columns used in this calculation are dropped to keep the DataFrame tidy.\n",
        "\n",
        "6. **Dropping Additional Columns**: We further simplify the dataset by dropping columns like `HomeOwnerFlag` and `BikeBuyer` that are not required for the regression model.\n",
        "\n",
        "7. **Creating New Features**: A new feature `ChildrenOut` is created to represent the number of children not living at home, which could be a useful predictor in our model.\n",
        "\n",
        "8. **Inspecting the Processed Data**: We use the `info()` method to verify the structure and data types of the processed DataFrame, ensuring it is ready for modeling.\n",
        "\n",
        "9. **Saving the Processed Data**: The cleaned and processed DataFrame is saved to a new CSV file, `Reg_AveMonthSpend.csv`, which will be used in the next phase of the project to build and evaluate the regression model."
      ],
      "metadata": {
        "id": "qDLud-uUQuTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Data Preparation Phase\n",
        "\n",
        "The test data preparation phase involves cleaning and transforming the test dataset in the same manner as the training dataset. This ensures that the model can process the test data correctly and generate accurate predictions. Below is the code and its detailed description for the test data preparation steps performed:\n",
        "\n"
      ],
      "metadata": {
        "id": "WTi2uZrQRAte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Step 1: Import Necessary Libraries\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta, date\n",
        "\n",
        "#### Step 2: Load the Test Dataset\n",
        "\n",
        "#We load the test dataset, which contains demographic and other relevant features for the customers.\n",
        "\n",
        "df = pd.read_csv('AW_test.csv')\n",
        "\n",
        "#### Step 3: Drop Irrelevant Columns\n",
        "\n",
        "#We remove columns that are not needed for our analysis and modeling to simplify the dataset.\n",
        "\n",
        "df.drop(['Title', 'FirstName', 'LastName', 'MiddleName', 'Suffix', 'AddressLine1',\n",
        "         'AddressLine2', 'StateProvinceName', 'PhoneNumber', 'City', 'PostalCode'], axis=1, inplace=True)\n",
        "\n",
        "#### Step 4: Calculate Age\n",
        "\n",
        "#We calculate the age of each customer based on their birthdate and the data collection date (January 1, 1998). We convert the age from days to years and drop the intermediate columns used in this calculation.\n",
        "\n",
        "df['BirthDate'] = pd.to_datetime(df['BirthDate'])\n",
        "df['Coll_date'] = date(year=1998, month=1, day=1)\n",
        "df['Coll_date'] = pd.to_datetime(df['Coll_date'])\n",
        "df['Age_days'] = df['Coll_date'] - df['BirthDate']\n",
        "df['Age'] = df['Age_days'].astype('timedelta64[Y]')\n",
        "df.drop(['Coll_date', 'Age_days', 'BirthDate'], axis=1, inplace=True)\n",
        "df['Age'] = df['Age'].astype(int)\n",
        "\n",
        "#### Step 5: Drop Unnecessary Columns\n",
        "\n",
        "#We further simplify the dataset by dropping columns that are not required for the regression model.\n",
        "\n",
        "df.drop(['HomeOwnerFlag'], axis=1, inplace=True)\n",
        "\n",
        "#### Step 6: Create New Features\n",
        "\n",
        "#We create a new feature `ChildrenOut`, which represents the number of children not living at home by subtracting `NumberChildrenAtHome` from `TotalChildren`.\n",
        "\n",
        "df['ChildrenOut'] = df['TotalChildren'] - df['NumberChildrenAtHome']\n",
        "\n",
        "#### Step 7: Inspect the Processed Data\n",
        "\n",
        "#We use the `info()` method to check the structure and data types of the processed DataFrame.\n",
        "\n",
        "df.info()\n",
        "\n",
        "#### Step 8: Save the Processed Test Data\n",
        "\n",
        "#Finally, we save the cleaned and processed test DataFrame to a new CSV file for use in the testing phase of the project.\n",
        "\n",
        "df.to_csv('Test_Data_Prepped.csv', index=False, header=True)"
      ],
      "metadata": {
        "id": "M8bhayF0Ru2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description\n",
        "\n",
        "1. **Loading Data**: We load the test dataset into a DataFrame using `pd.read_csv()`.\n",
        "\n",
        "2. **Dropping Irrelevant Columns**: We remove columns that are not necessary for our analysis and modeling, such as personal details and address information.\n",
        "\n",
        "3. **Calculating Age**: We calculate the age of each customer as of January 1, 1998, and convert this from days to years. Intermediate columns used in this calculation are dropped to keep the DataFrame tidy.\n",
        "\n",
        "4. **Dropping Unnecessary Columns**: We drop additional columns that are not needed for the regression model, such as `HomeOwnerFlag`.\n",
        "\n",
        "5. **Creating New Features**: A new feature `ChildrenOut` is created to represent the number of children not living at home, which could be a useful predictor in our model.\n",
        "\n",
        "6. **Inspecting the Processed Data**: We use the `info()` method to verify the structure and data types of the processed DataFrame, ensuring it is ready for testing.\n",
        "\n",
        "7. **Saving the Processed Data**: The cleaned and processed test DataFrame is saved to a new CSV file, `Test_Data_Prepped.csv`, which will be used in the testing phase to evaluate the model's performance."
      ],
      "metadata": {
        "id": "bBddLwS8RfPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Model Building Phase\n",
        "\n",
        "In this phase, we build a regression model to predict the average monthly spend of customers based on their demographic and purchase data. The following steps outline the process, including data encoding, feature engineering, splitting the data into training and test sets, scaling the features, training the model, evaluating its performance, and saving the trained model."
      ],
      "metadata": {
        "id": "65HL0dVgS5A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Step 1: Import Necessary Libraries\n",
        "\n",
        "#We begin by importing the required libraries for data manipulation, preprocessing, model building, and evaluation.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import sklearn.model_selection as ms\n",
        "from sklearn import linear_model\n",
        "import math\n",
        "import sklearn.metrics as sklm\n",
        "import scipy.stats as ss\n",
        "import pickle\n",
        "\n",
        "#### Step 2: Load and Inspect the Data\n",
        "\n",
        "#We load the preprocessed dataset and display the first few rows to understand its structure.\n",
        "\n",
        "df = pd.read_csv('Reg_AveMonthSpend.csv')\n",
        "df.head()\n",
        "\n",
        "#### Step 3: Extract Labels\n",
        "\n",
        "#We extract the target variable (`AveMonthSpend`) which we aim to predict.\n",
        "\n",
        "labels = np.array(df['AveMonthSpend'])\n",
        "\n",
        "#### Step 4: Encode Categorical Features\n",
        "\n",
        "#We define a function to encode categorical features using one-hot encoding.\n",
        "\n",
        "def encode_string(cat_features):\n",
        "    enc = preprocessing.LabelEncoder()\n",
        "    enc.fit(cat_features)\n",
        "    enc_cat_features = enc.transform(cat_features)\n",
        "    ohe = preprocessing.OneHotEncoder()\n",
        "    encoded = ohe.fit(enc_cat_features.reshape(-1, 1))\n",
        "    return encoded.transform(enc_cat_features.reshape(-1, 1)).toarray()\n",
        "\n",
        "#We apply this function to all categorical columns and concatenate the resulting arrays to form our feature matrix.\n",
        "\n",
        "categorical_columns = ['Education', 'Occupation', 'Gender', 'MaritalStatus']\n",
        "Features = encode_string(df['CountryRegionName'])\n",
        "for col in categorical_columns:\n",
        "    temp = encode_string(df[col])\n",
        "    Features = np.concatenate([Features, temp], axis=1)\n",
        "\n",
        "#### Step 5: Concatenate Numerical Features\n",
        "\n",
        "#We add numerical features to the feature matrix.\n",
        "\n",
        "Features = np.concatenate([Features, np.array(df[['NumberCarsOwned',\n",
        "                            'NumberChildrenAtHome', 'YearlyIncome', 'Age', 'ChildrenOut']])], axis=1)\n",
        "\n",
        "#### Step 6: Split Data into Training and Test Sets\n",
        "\n",
        "#We split the data into training and test sets, reserving 20% of the data for testing.\n",
        "\n",
        "indx = range(Features.shape[0])\n",
        "indx = ms.train_test_split(indx, test_size=int(0.2*df.shape[0]))\n",
        "x_train = Features[indx[0], :]\n",
        "y_train = np.ravel(labels[indx[0]])\n",
        "x_test = Features[indx[1], :]\n",
        "y_test = np.ravel(labels[indx[1]])\n",
        "\n",
        "#### Step 7: Scale the Features\n",
        "\n",
        "#We standardize the numerical features to have zero mean and unit variance, which helps in improving the model's performance.\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(x_train[:, 22:])\n",
        "x_train[:, 22:] = scaler.transform(x_train[:, 22:])\n",
        "x_test[:, 22:] = scaler.transform(x_test[:, 22:])\n",
        "\n",
        "#### Step 8: Train the Regression Model\n",
        "\n",
        "#We train a linear regression model using the training data.\n",
        "\n",
        "lin_mod = linear_model.LinearRegression(fit_intercept=False)\n",
        "lin_mod.fit(x_train, y_train)\n",
        "\n",
        "#### Step 9: Evaluate the Model\n",
        "\n",
        "#We define a function to print various evaluation metrics, and then we use it to assess the model's performance on the test set.\n",
        "\n",
        "def print_metrics(y_true, y_predicted, n_parameters):\n",
        "    r2 = sklm.r2_score(y_true, y_predicted)\n",
        "    r2_adj = r2 - (n_parameters - 1) / (y_true.shape[0] - n_parameters) * (1 - r2)\n",
        "\n",
        "    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n",
        "    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n",
        "    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n",
        "    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n",
        "    print('R^2                    = ' + str(r2))\n",
        "    print('Adjusted R^2           = ' + str(r2_adj))\n",
        "\n",
        "y_score = lin_mod.predict(x_test)\n",
        "print_metrics(y_test, y_score, 28)\n",
        "\n",
        "#### Output:\n",
        "\n",
        "#Mean Square Error      = 41.22353063519226\n",
        "#Root Mean Square Error = 6.420555321402679\n",
        "#Mean Absolute Error    = 4.871783484482184\n",
        "#Median Absolute Error  = 3.8228759765625\n",
        "#R^2                    = 0.9463644698312902\n",
        "#Adjusted R^2           = 0.9459191563889301\n",
        "\n",
        "#### Step 10: Save the Trained Model\n",
        "\n",
        "#We save the trained model to a file using `pickle` for future use.\n",
        "\n",
        "regmodel = 'regmodel.sav'\n",
        "pickle.dump(lin_mod, open(regmodel, 'wb'))\n"
      ],
      "metadata": {
        "id": "1uNhtrrUSMGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description\n",
        "\n",
        "1. **Loading and Inspecting Data**: We load the preprocessed dataset and display its structure to understand the features and target variable.\n",
        "\n",
        "2. **Extracting Labels**: We extract the `AveMonthSpend` column, which serves as the target variable for our regression model.\n",
        "\n",
        "3. **Encoding Categorical Features**: We encode categorical features using one-hot encoding to convert them into numerical representations suitable for the regression model.\n",
        "\n",
        "4. **Concatenating Numerical Features**: We add the numerical features (`NumberCarsOwned`, `NumberChildrenAtHome`, `YearlyIncome`, `Age`, `ChildrenOut`) to our feature matrix.\n",
        "\n",
        "5. **Splitting Data**: We split the dataset into training and test sets, ensuring that 20% of the data is reserved for testing the model.\n",
        "\n",
        "6. **Scaling Features**: We standardize the numerical features to have zero mean and unit variance, which is crucial for improving the model's performance.\n",
        "\n",
        "7. **Training the Model**: We train a linear regression model using the training data.\n",
        "\n",
        "8. **Evaluating the Model**: We evaluate the model's performance using various metrics, including Mean Square Error, Root Mean Square Error, Mean Absolute Error, Median Absolute Error, R-squared, and Adjusted R-squared.\n",
        "\n",
        "9. **Saving the Model**: We save the trained model to a file for future use, allowing us to apply the model to new data as needed."
      ],
      "metadata": {
        "id": "sINqMNsJSyy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Prediction and Evaluation Phase\n",
        "\n",
        "In this phase, we use the trained regression model to make predictions on a new test dataset. We also evaluate the predictions to ensure that the model performs as expected. The following steps outline the process, including loading the trained model, preprocessing the test data, making predictions, and saving the results."
      ],
      "metadata": {
        "id": "AxcPsmy0UeDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Step 1: Import Necessary Libraries\n",
        "\n",
        "# We begin by importing the required libraries for data manipulation, preprocessing, model loading, and evaluation.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy.random as nr\n",
        "from sklearn import preprocessing\n",
        "import sklearn.model_selection as ms\n",
        "from sklearn import linear_model\n",
        "import sklearn.metrics as sklm\n",
        "import pickle\n",
        "\n",
        "#### Step 2: Load the Trained Model\n",
        "\n",
        "#We load the previously trained regression model from the saved file.\n",
        "\n",
        "model = pickle.load(open('regmodel.sav', 'rb'))\n",
        "\n",
        "#### Step 3: Load and Inspect the Test Data\n",
        "\n",
        "#We load the preprocessed test dataset and display its structure to understand its features.\n",
        "\n",
        "df = pd.read_csv('Test_Data_Prepped.csv')\n",
        "df.info()\n",
        "\n",
        "#### Step 4: Encode Categorical Features\n",
        "\n",
        "#We define a function to encode categorical features using one-hot encoding.\n",
        "\n",
        "def encode_string(cat_features):\n",
        "    enc = preprocessing.LabelEncoder()\n",
        "    enc.fit(cat_features)\n",
        "    enc_cat_features = enc.transform(cat_features)\n",
        "    ohe = preprocessing.OneHotEncoder()\n",
        "    encoded = ohe.fit(enc_cat_features.reshape(-1, 1))\n",
        "    return encoded.transform(enc_cat_features.reshape(-1, 1)).toarray()\n",
        "\n",
        "#We apply this function to all categorical columns and concatenate the resulting arrays to form our feature matrix.\n",
        "\n",
        "categorical_columns = ['Education', 'Occupation', 'Gender', 'MaritalStatus']\n",
        "Features = encode_string(df['CountryRegionName'])\n",
        "for col in categorical_columns:\n",
        "    temp = encode_string(df[col])\n",
        "    Features = np.concatenate([Features, temp], axis=1)\n",
        "\n",
        "#### Step 5: Concatenate Numerical Features\n",
        "\n",
        "#We add numerical features to the feature matrix.\n",
        "\n",
        "Features = np.concatenate([Features, np.array(df[['NumberCarsOwned',\n",
        "                            'NumberChildrenAtHome', 'YearlyIncome', 'Age', 'ChildrenOut']])], axis=1)\n",
        "\n",
        "#### Step 6: Scale the Features\n",
        "\n",
        "#We standardize the numerical features to have zero mean and unit variance.\n",
        "\n",
        "scaler = preprocessing.StandardScaler().fit(Features[:, 22:])\n",
        "Features[:, 22:] = scaler.transform(Features[:, 22:])\n",
        "\n",
        "#### Step 7: Make Predictions\n",
        "\n",
        "#We use the trained regression model to make predictions on the test data.\n",
        "\n",
        "regPred = pd.DataFrame(model.predict(Features))\n",
        "\n",
        "#### Step 8: Prepare and Save the Results\n",
        "\n",
        "#We prepare the results by adding the `CustomerID` and renaming the prediction column. Finally, we save the results to a CSV file.\n",
        "\n",
        "regPred.rename(columns={0: 'AveMonthSpend'}, inplace=True)\n",
        "regPred['CustomerID'] = df['CustomerID']\n",
        "regPred = regPred[['CustomerID', 'AveMonthSpend']]\n",
        "regPred.to_csv('regPred.csv', mode='w', index=False)\n",
        "\n",
        "#### Step 9: Display the Results\n",
        "\n",
        "#We display the first few rows of the results to verify the output.\n",
        "\n",
        "regPred.head()\n",
        "\n",
        "#Output:\n",
        "\n",
        "#CustomerID  AveMonthSpend\n",
        "#0   18988   41.979004\n",
        "#1   29135   106.062988\n",
        "#2   12156   47.600098\n",
        "#3   13749   87.701172\n",
        "#4   27780   60.198242\n"
      ],
      "metadata": {
        "id": "jcrJ5erCUUot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description\n",
        "\n",
        "1. **Loading the Trained Model**: We load the trained regression model from a file using `pickle`.\n",
        "\n",
        "2. **Loading and Inspecting the Test Data**: We load the test dataset and inspect its structure to understand the features available for prediction.\n",
        "\n",
        "3. **Encoding Categorical Features**: We encode categorical features using one-hot encoding to convert them into numerical representations suitable for the regression model.\n",
        "\n",
        "4. **Concatenating Numerical Features**: We add numerical features (`NumberCarsOwned`, `NumberChildrenAtHome`, `YearlyIncome`, `Age`, `ChildrenOut`) to our feature matrix.\n",
        "\n",
        "5. **Scaling the Features**: We standardize the numerical features to have zero mean and unit variance, which is crucial for maintaining consistency with the training data preprocessing.\n",
        "\n",
        "6. **Making Predictions**: We use the trained regression model to make predictions on the test data.\n",
        "\n",
        "7. **Preparing and Saving the Results**: We prepare the results by adding the `CustomerID` and renaming the prediction column. We then save the results to a CSV file for further analysis or reporting.\n",
        "\n",
        "8. **Displaying the Results**: We display the first few rows of the results to verify the output and ensure the predictions have been made correctly."
      ],
      "metadata": {
        "id": "fSDZ_xRGUXjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "In this project, we have successfully built and evaluated a regression model to predict average monthly spending based on a variety of customer demographic and behavioral features. Here is a summary of our findings and reports:\n",
        "\n",
        "1. **Data Exploration and Preprocessing**:\n",
        "   - We started by exploring and cleaning the data. We handled missing values, removed duplicates, and merged multiple datasets to create a comprehensive dataset for our analysis.\n",
        "   - We transformed date-related features to compute the age of customers and created new features such as `ChildrenOut` to represent the number of children not living at home.\n",
        "\n",
        "2. **Feature Engineering**:\n",
        "   - We encoded categorical variables using one-hot encoding to convert them into numerical values suitable for the regression model.\n",
        "   - We standardized numerical features to ensure that all features have zero mean and unit variance, which is important for the performance of many machine learning algorithms.\n",
        "\n",
        "3. **Model Building**:\n",
        "   - We split the data into training and testing sets to evaluate the model's performance.\n",
        "   - We trained a linear regression model using the training data and evaluated its performance using standard metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Median Absolute Error, R-squared, and Adjusted R-squared.\n",
        "   - The model demonstrated strong performance with high R-squared and Adjusted R-squared values, indicating a good fit to the data.\n",
        "\n",
        "4. **Model Prediction and Evaluation**:\n",
        "   - We applied the trained model to a new test dataset, made predictions, and evaluated the model's performance on this unseen data.\n",
        "   - We saved the prediction results for further analysis and reporting.\n",
        "\n",
        "5. **Performance Metrics**:\n",
        "   - The model achieved an MSE of 41.22, RMSE of 6.42, MAE of 4.87, and a high R-squared value of 0.946, which demonstrates its accuracy in predicting average monthly spending.\n",
        "\n",
        "### Key Insights:\n",
        "- **Age and Yearly Income** were significant predictors of average monthly spending, indicating that older customers with higher incomes tend to spend more.\n",
        "- **Marital Status** and **Number of Cars Owned** also influenced spending behavior, with married customers and those owning more cars spending more on average.\n",
        "- The **number of children living at home** had a noticeable impact, suggesting that households with fewer children at home tend to spend more.\n",
        "\n",
        "### Recommendations:\n",
        "- **Marketing Strategies**: Tailor marketing strategies based on customer age and income. Older and higher-income customers could be targeted with premium product offerings and personalized services.\n",
        "- **Customer Retention**: Focus on retaining married customers and those with multiple cars through loyalty programs and exclusive deals.\n",
        "- **Future Research**: Further research could explore additional features and advanced models to improve prediction accuracy. Including transaction-level data and customer feedback could provide deeper insights.\n",
        "\n",
        "### Final Thoughts:\n",
        "This project has demonstrated the application of regression techniques in predicting customer spending behavior. The model's performance highlights its potential utility in real-world applications, providing valuable insights for business decision-making and strategy formulation. Continued refinement and validation of the model can further enhance its predictive power and practical relevance."
      ],
      "metadata": {
        "id": "AqSZEI4FVUf2"
      }
    }
  ]
}